{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.1104 \n",
      "After 1000 training step(s), validation accuracy using average model is 0.9768 \n",
      "After 2000 training step(s), validation accuracy using average model is 0.9792 \n",
      "After 3000 training step(s), validation accuracy using average model is 0.9814 \n",
      "After 4000 training step(s), validation accuracy using average model is 0.9818 \n",
      "After 5000 training step(s), validation accuracy using average model is 0.982 \n",
      "After 6000 training step(s), validation accuracy using average model is 0.9822 \n",
      "After 7000 training step(s), validation accuracy using average model is 0.9838 \n",
      "After 8000 training step(s), validation accuracy using average model is 0.9834 \n",
      "After 9000 training step(s), validation accuracy using average model is 0.9836 \n",
      "After 10000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 11000 training step(s), validation accuracy using average model is 0.9844 \n",
      "After 12000 training step(s), validation accuracy using average model is 0.9836 \n",
      "After 13000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 14000 training step(s), validation accuracy using average model is 0.9842 \n",
      "After 15000 training step(s), validation accuracy using average model is 0.9848 \n",
      "After 16000 training step(s), validation accuracy using average model is 0.9844 \n",
      "After 17000 training step(s), validation accuracy using average model is 0.985 \n",
      "After 18000 training step(s), validation accuracy using average model is 0.9842 \n",
      "After 19000 training step(s), validation accuracy using average model is 0.9856 \n",
      "After 20000 training step(s), validation accuracy using average model is 0.9848 \n",
      "After 21000 training step(s), validation accuracy using average model is 0.985 \n",
      "After 22000 training step(s), validation accuracy using average model is 0.9844 \n",
      "After 23000 training step(s), validation accuracy using average model is 0.985 \n",
      "After 24000 training step(s), validation accuracy using average model is 0.9854 \n",
      "After 25000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 26000 training step(s), validation accuracy using average model is 0.9842 \n",
      "After 27000 training step(s), validation accuracy using average model is 0.9852 \n",
      "After 28000 training step(s), validation accuracy using average model is 0.9846 \n",
      "After 29000 training step(s), validation accuracy using average model is 0.9852 \n",
      "After 30000 training step(s), test accuracy using average model is 0.9846\n",
      "运行时间: 664.111930847168\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "INPUT_NODE = 784 # 输入层节点数，即图片像素\n",
    "OUTPUT_NODE = 10 # 输出层节点数；输出的是10*1的向量，可参考前文的Example training data label\n",
    "\n",
    "LAYER1_NODE = 500 #隐藏层节点数\n",
    "\n",
    "BATCH_SIZE = 100 # 一次训练batch中的数据个数；数字越小（极限为1）则越接近随机梯度下降，越大则越接近梯度下降\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8     # 基础学习率\n",
    "LEARNING_RATE_DECAY = 0.99   # 学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001 # 描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 30000       # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率\n",
    "\n",
    "\n",
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 若没有提供移动平均类，则直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    # 若提供了滑动平均类，则首先使用avg_class.average函数计算得出变量的滑动平均值\n",
    "    # 然后再计算相应的神经网络的前向传播结果\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "# 模型训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    # 初始化生成隐藏层的参数，这里用truncated_normal而非普通normal，是为了加速训练过程\n",
    "    # 注：tf.truncated_normal函数的效果是如得到的随机值偏离均值2个标准差以上，则重新随机一次直至在2个标准差以内\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    # 初始化生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算在当前参数下前向传播的效果，这里滑动平均类为None所以函数不会使用参数的滑动平均值\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 定义存储训练轮数的变量。这个变量不需要计算滑动平均值，所以这里设定为不可训练变量（trainable=False）\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 给定滑动拼接衰减率和训练轮数的变量，初始化滑动平均类；在第4章中介绍过给定训练轮数的变量可加快训练早期变量的更新速度\n",
    "    variale_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # 对所有神经网络参数的变量上使用滑动平均（不可训练变量除外）\n",
    "    variale_averages_op = variale_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # 计算使用了滑动平均之后的前向传播效果\n",
    "    average_y = inference(x, variale_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 计算交叉熵；因为one_hot=True，对于稀疏矩阵可用这个函数来加速交叉熵的计算\n",
    "    # 【勘误】注意这里原书代码有误\n",
    "    # 原书是cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))可能跑不通\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "    # 计算当前batch中所有样例的交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    #总损失 = 交叉熵损失 + 正则化损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,     # 基础学习率，随着迭代的进行、更新变量时使用的学习率在此基础上递减\n",
    "        global_step,            # 当前迭代轮次\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # 做完所有训练需要的总轮次\n",
    "        LEARNING_RATE_DECAY,    # 学习率衰减速度\n",
    "        staircase=True)\n",
    "\n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variale_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 分别准备验证集和测试集数据\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证集上的结果\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    start_time=time.time()\n",
    "    main()\n",
    "    print('运行时间:',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无正则化模型\n",
    "## loss = cross_entropy_mean + regularization修改为\n",
    "## loss = cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "INPUT_NODE = 784 # 输入层节点数，即图片像素\n",
    "OUTPUT_NODE = 10 # 输出层节点数；输出的是10*1的向量，可参考前文的Example training data label\n",
    "\n",
    "LAYER1_NODE = 500 #隐藏层节点数\n",
    "\n",
    "BATCH_SIZE = 100 # 一次训练batch中的数据个数；数字越小（极限为1）则越接近随机梯度下降，越大则越接近梯度下降\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8     # 基础学习率\n",
    "LEARNING_RATE_DECAY = 0.99   # 学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001 # 描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 30000       # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率\n",
    "\n",
    "\n",
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 若没有提供移动平均类，则直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    # 若提供了滑动平均类，则首先使用avg_class.average函数计算得出变量的滑动平均值\n",
    "    # 然后再计算相应的神经网络的前向传播结果\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "# 模型训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    # 初始化生成隐藏层的参数，这里用truncated_normal而非普通normal，是为了加速训练过程\n",
    "    # 注：tf.truncated_normal函数的效果是如得到的随机值偏离均值2个标准差以上，则重新随机一次直至在2个标准差以内\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    # 初始化生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算在当前参数下前向传播的效果，这里滑动平均类为None所以函数不会使用参数的滑动平均值\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 定义存储训练轮数的变量。这个变量不需要计算滑动平均值，所以这里设定为不可训练变量（trainable=False）\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 给定滑动拼接衰减率和训练轮数的变量，初始化滑动平均类；在第4章中介绍过给定训练轮数的变量可加快训练早期变量的更新速度\n",
    "    variale_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # 对所有神经网络参数的变量上使用滑动平均（不可训练变量除外）\n",
    "    variale_averages_op = variale_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # 计算使用了滑动平均之后的前向传播效果\n",
    "    average_y = inference(x, variale_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 计算交叉熵；因为one_hot=True，对于稀疏矩阵可用这个函数来加速交叉熵的计算\n",
    "    # 【勘误】注意这里原书代码有误\n",
    "    # 原书是cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))可能跑不通\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "    # 计算当前batch中所有样例的交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    #总损失 = 交叉熵损失 + 正则化损失\n",
    "#     loss = cross_entropy_mean + regularization\n",
    "    loss = cross_entropy_mean\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,     # 基础学习率，随着迭代的进行、更新变量时使用的学习率在此基础上递减\n",
    "        global_step,            # 当前迭代轮次\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # 做完所有训练需要的总轮次\n",
    "        LEARNING_RATE_DECAY,    # 学习率衰减速度\n",
    "        staircase=True)\n",
    "\n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variale_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 分别准备验证集和测试集数据\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证集上的结果\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    start_time=time.time()\n",
    "    main()\n",
    "    print('运行时间:',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 固定学习率(而非指数衰减)\n",
    "## 1) 将学习率LEARNING_RATE固定为0.1，\n",
    "## 2) 在tf.train.GradientDescentOptimizer之中将学习率就设定为固定的LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "INPUT_NODE = 784 # 输入层节点数，即图片像素\n",
    "OUTPUT_NODE = 10 # 输出层节点数；输出的是10*1的向量，可参考前文的Example training data label\n",
    "\n",
    "LAYER1_NODE = 500 #隐藏层节点数\n",
    "\n",
    "BATCH_SIZE = 100 # 一次训练batch中的数据个数；数字越小（极限为1）则越接近随机梯度下降，越大则越接近梯度下降\n",
    "\n",
    "# LEARNING_RATE_BASE = 0.8     # 基础学习率\n",
    "# LEARNING_RATE_DECAY = 0.99   # 学习率的衰减率\n",
    "LEARING_RATE=0.1 # 固定的学习率\n",
    "REGULARIZATION_RATE = 0.0001 # 描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 30000       # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率\n",
    "\n",
    "\n",
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 若没有提供移动平均类，则直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    # 若提供了滑动平均类，则首先使用avg_class.average函数计算得出变量的滑动平均值\n",
    "    # 然后再计算相应的神经网络的前向传播结果\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "# 模型训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    # 初始化生成隐藏层的参数，这里用truncated_normal而非普通normal，是为了加速训练过程\n",
    "    # 注：tf.truncated_normal函数的效果是如得到的随机值偏离均值2个标准差以上，则重新随机一次直至在2个标准差以内\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    # 初始化生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算在当前参数下前向传播的效果，这里滑动平均类为None所以函数不会使用参数的滑动平均值\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 定义存储训练轮数的变量。这个变量不需要计算滑动平均值，所以这里设定为不可训练变量（trainable=False）\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 给定滑动拼接衰减率和训练轮数的变量，初始化滑动平均类；在第4章中介绍过给定训练轮数的变量可加快训练早期变量的更新速度\n",
    "    variale_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # 对所有神经网络参数的变量上使用滑动平均（不可训练变量除外）\n",
    "    variale_averages_op = variale_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # 计算使用了滑动平均之后的前向传播效果\n",
    "    average_y = inference(x, variale_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 计算交叉熵；因为one_hot=True，对于稀疏矩阵可用这个函数来加速交叉熵的计算\n",
    "    # 【勘误】注意这里原书代码有误\n",
    "    # 原书是cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))可能跑不通\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "    # 计算当前batch中所有样例的交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    #总损失 = 交叉熵损失 + 正则化损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,     # 基础学习率，随着迭代的进行、更新变量时使用的学习率在此基础上递减\n",
    "        global_step,            # 当前迭代轮次\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # 做完所有训练需要的总轮次\n",
    "        LEARNING_RATE_DECAY,    # 学习率衰减速度\n",
    "        staircase=True)\n",
    "\n",
    "    # 优化损失函数\n",
    "#     train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    train_step = tf.train.GradientDescentOptimizer(LEARING_RATE)\n",
    "    \n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variale_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 分别准备验证集和测试集数据\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证集上的结果\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    start_time=time.time()\n",
    "    main()\n",
    "    print('运行时间:',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无激活函数\n",
    "## 在inference函数（计算前向传播的值）中，不使用2中给的ReLU激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "INPUT_NODE = 784 # 输入层节点数，即图片像素\n",
    "OUTPUT_NODE = 10 # 输出层节点数；输出的是10*1的向量，可参考前文的Example training data label\n",
    "\n",
    "LAYER1_NODE = 500 #隐藏层节点数\n",
    "\n",
    "BATCH_SIZE = 100 # 一次训练batch中的数据个数；数字越小（极限为1）则越接近随机梯度下降，越大则越接近梯度下降\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8     # 基础学习率\n",
    "LEARNING_RATE_DECAY = 0.99   # 学习率的衰减率\n",
    "REGULARIZATION_RATE = 0.0001 # 描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 30000       # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99  # 滑动平均衰减率\n",
    "\n",
    "\n",
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果\n",
    "# def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "#     # 若没有提供移动平均类，则直接使用参数当前的取值\n",
    "#     if avg_class == None:\n",
    "#         layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "#         return tf.matmul(layer1, weights2) + biases2\n",
    "#     # 若提供了滑动平均类，则首先使用avg_class.average函数计算得出变量的滑动平均值\n",
    "#     # 然后再计算相应的神经网络的前向传播结果\n",
    "#     else:\n",
    "#         layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "#         return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.matmul(input_tensor, weights1) + biases1\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        layer1 = tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1)\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n",
    "\n",
    "\n",
    "# 模型训练过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    # 初始化生成隐藏层的参数，这里用truncated_normal而非普通normal，是为了加速训练过程\n",
    "    # 注：tf.truncated_normal函数的效果是如得到的随机值偏离均值2个标准差以上，则重新随机一次直至在2个标准差以内\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n",
    "\n",
    "    # 初始化生成输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    # 计算在当前参数下前向传播的效果，这里滑动平均类为None所以函数不会使用参数的滑动平均值\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 定义存储训练轮数的变量。这个变量不需要计算滑动平均值，所以这里设定为不可训练变量（trainable=False）\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 给定滑动拼接衰减率和训练轮数的变量，初始化滑动平均类；在第4章中介绍过给定训练轮数的变量可加快训练早期变量的更新速度\n",
    "    variale_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "    # 对所有神经网络参数的变量上使用滑动平均（不可训练变量除外）\n",
    "    variale_averages_op = variale_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    # 计算使用了滑动平均之后的前向传播效果\n",
    "    average_y = inference(x, variale_averages, weights1, biases1, weights2, biases2)\n",
    "\n",
    "    # 计算交叉熵；因为one_hot=True，对于稀疏矩阵可用这个函数来加速交叉熵的计算\n",
    "    # 【勘误】注意这里原书代码有误\n",
    "    # 原书是cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))可能跑不通\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "\n",
    "    # 计算当前batch中所有样例的交叉熵的平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    #总损失 = 交叉熵损失 + 正则化损失\n",
    "    loss = cross_entropy_mean + regularization\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,     # 基础学习率，随着迭代的进行、更新变量时使用的学习率在此基础上递减\n",
    "        global_step,            # 当前迭代轮次\n",
    "        mnist.train.num_examples / BATCH_SIZE,  # 做完所有训练需要的总轮次\n",
    "        LEARNING_RATE_DECAY,    # 学习率衰减速度\n",
    "        staircase=True)\n",
    "\n",
    "    # 优化损失函数\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # 反向传播更新参数和更新每一个参数的滑动平均值\n",
    "    with tf.control_dependencies([train_step, variale_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # 计算正确率\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        # 分别准备验证集和测试集数据\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证集上的结果\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "# 主程序\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    start_time=time.time()\n",
    "    main()\n",
    "    print('运行时间:',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 无隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "INPUT_NODE = 784\n",
    "OUTPUT_NODE = 10\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "REGULARIZATION_RATE = 0.0001\n",
    "TRAINING_STEPS = 30000\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "\n",
    "\n",
    "# 删除weights2和biases2，直接以一层输出\n",
    "def inference(input_tensor, avg_class, weights1, biases1):\n",
    "    if avg_class == None:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        return layer1\n",
    "    else:\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return layer1\n",
    "\n",
    "\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "\n",
    "    # 仅保留weights1和biases1，且直接按OUTPUT_NODE输出\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "\n",
    "    y = inference(x, None, weights1, biases1)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    variale_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    variale_averages_op = variale_averages.apply(tf.trainable_variables())\n",
    "    average_y = inference(x, variale_averages, weights1, biases1)\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n",
    "    regularization = regularizer(weights1) # 这里删去对weights2的正则化\n",
    "    loss = cross_entropy_mean + regularization\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE,\n",
    "        LEARNING_RATE_DECAY, staircase=True)\n",
    "\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    with tf.control_dependencies([train_step, variale_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        validate_feed = {x: mnist.validation.images,\n",
    "                         y_: mnist.validation.labels}\n",
    "        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n",
    "\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            if i % 1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g \" % (i, validate_acc))\n",
    "\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x: xs, y_: ys})\n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print((\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc)))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
